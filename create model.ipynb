{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', 'brush.png', 'brush.xcf', 'create model.ipynb', 'Paint.ipynb', 'screenshot.png', 'test', 'Test Network.ipynb', 'train', 'validation']\n",
      "C:\\Users\\karan\\Desktop\\Projects\\Digit recognition\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "\n",
    "import random\n",
    "import os\n",
    "print(os.listdir(os.getcwd()))\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training zero images: 200\n",
      "Total training one images: 200\n",
      "Total training two images: 200\n",
      "Total training three images: 200\n",
      "Total training four images: 200\n",
      "Total training five images: 200\n",
      "Total training six images: 200\n",
      "Total training seven images: 200\n",
      "Total training eight images: 200\n",
      "Total training nine images: 200\n"
     ]
    }
   ],
   "source": [
    "zero_dir = os.path.join('C:\\\\Users\\\\karan\\\\Desktop\\\\Projects\\\\Digit recognition\\\\train\\\\zero')\n",
    "one_dir = os.path.join('C:\\\\Users\\\\karan\\\\Desktop\\\\Projects\\\\Digit recognition\\\\train\\\\one')\n",
    "two_dir = os.path.join('C:\\\\Users\\\\karan\\\\Desktop\\\\Projects\\\\Digit recognition\\\\train\\\\two')\n",
    "three_dir = os.path.join('C:\\\\Users\\\\karan\\\\Desktop\\\\Projects\\\\Digit recognition\\\\train\\\\three')\n",
    "four_dir = os.path.join('C:\\\\Users\\\\karan\\\\Desktop\\\\Projects\\\\Digit recognition\\\\train\\\\four')\n",
    "five_dir = os.path.join('C:\\\\Users\\\\karan\\\\Desktop\\\\Projects\\\\Digit recognition\\\\train\\\\five')\n",
    "six_dir = os.path.join('C:\\\\Users\\\\karan\\\\Desktop\\\\Projects\\\\Digit recognition\\\\train\\\\six')\n",
    "seven_dir = os.path.join('C:\\\\Users\\\\karan\\\\Desktop\\\\Projects\\\\Digit recognition\\\\train\\\\seven')\n",
    "eight_dir = os.path.join('C:\\\\Users\\\\karan\\\\Desktop\\\\Projects\\\\Digit recognition\\\\train\\\\eight')\n",
    "nine_dir = os.path.join('C:\\\\Users\\\\karan\\\\Desktop\\\\Projects\\\\Digit recognition\\\\train\\\\nine')\n",
    "\n",
    "print('Total training zero images:', len(os.listdir(zero_dir)))\n",
    "print('Total training one images:', len(os.listdir(one_dir)))\n",
    "print('Total training two images:', len(os.listdir(two_dir)))\n",
    "print('Total training three images:', len(os.listdir(three_dir)))\n",
    "print('Total training four images:', len(os.listdir(four_dir)))\n",
    "print('Total training five images:', len(os.listdir(five_dir)))\n",
    "print('Total training six images:', len(os.listdir(six_dir)))\n",
    "print('Total training seven images:', len(os.listdir(seven_dir)))\n",
    "print('Total training eight images:', len(os.listdir(eight_dir)))\n",
    "print('Total training nine images:', len(os.listdir(nine_dir)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['zero0.png', 'zero1.png', 'zero10.png', 'zero100.png', 'zero101.png', 'zero102.png', 'zero103.png', 'zero104.png', 'zero105.png', 'zero106.png']\n",
      "['one0.png', 'one1.png', 'one10.png', 'one100.png', 'one101.png', 'one102.png', 'one103.png', 'one104.png', 'one105.png', 'one106.png']\n",
      "['two0.png', 'two1.png', 'two10.png', 'two100.png', 'two101.png', 'two102.png', 'two103.png', 'two104.png', 'two105.png', 'two106.png']\n",
      "['three0.png', 'three1.png', 'three10.png', 'three100.png', 'three101.png', 'three102.png', 'three103.png', 'three104.png', 'three105.png', 'three106.png']\n",
      "['four0.png', 'four1.png', 'four10.png', 'four100.png', 'four101.png', 'four102.png', 'four103.png', 'four104.png', 'four105.png', 'four106.png']\n",
      "['five0.png', 'five1.png', 'five10.png', 'five100.png', 'five101.png', 'five102.png', 'five103.png', 'five104.png', 'five105.png', 'five106.png']\n",
      "['six0.png', 'six1.png', 'six10.png', 'six100.png', 'six101.png', 'six102.png', 'six103.png', 'six104.png', 'six105.png', 'six106.png']\n",
      "['seven0.png', 'seven1.png', 'seven10.png', 'seven100.png', 'seven101.png', 'seven102.png', 'seven103.png', 'seven104.png', 'seven105.png', 'seven106.png']\n",
      "['eight0.png', 'eight1.png', 'eight10.png', 'eight100.png', 'eight101.png', 'eight102.png', 'eight103.png', 'eight104.png', 'eight105.png', 'eight106.png']\n",
      "['nine0.png', 'nine1.png', 'nine10.png', 'nine100.png', 'nine101.png', 'nine102.png', 'nine103.png', 'nine104.png', 'nine105.png', 'nine106.png']\n"
     ]
    }
   ],
   "source": [
    "zero_files = os.listdir(zero_dir)\n",
    "print(zero_files[:10])\n",
    "\n",
    "one_files = os.listdir(one_dir)\n",
    "print(one_files[:10])\n",
    "\n",
    "two_files = os.listdir(two_dir)\n",
    "print(two_files[:10])\n",
    "\n",
    "three_files = os.listdir(three_dir)\n",
    "print(three_files[:10])\n",
    "\n",
    "four_files = os.listdir(four_dir)\n",
    "print(four_files[:10])\n",
    "\n",
    "five_files = os.listdir(five_dir)\n",
    "print(five_files[:10])\n",
    "\n",
    "six_files = os.listdir(six_dir)\n",
    "print(six_files[:10])\n",
    "\n",
    "seven_files = os.listdir(seven_dir)\n",
    "print(seven_files[:10])\n",
    "\n",
    "eight_files = os.listdir(eight_dir)\n",
    "print(eight_files[:10])\n",
    "\n",
    "nine_files = os.listdir(nine_dir)\n",
    "print(nine_files[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 10 classes.\n",
      "Found 200 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "TRAINING_DIR = \"C:\\\\Users\\\\karan\\\\Desktop\\\\Projects\\\\Digit recognition\\\\train\\\\\"\n",
    "training_datagen = ImageDataGenerator(\n",
    "    rescale = 1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest')\n",
    "\n",
    "VALIDATION_DIR = \"C:\\\\Users\\\\karan\\\\Desktop\\\\Projects\\\\Digit recognition\\\\test\\\\\"\n",
    "validation_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "train_generator = training_datagen.flow_from_directory(\n",
    "    TRAINING_DIR,\n",
    "    target_size=(150,150),\n",
    "    class_mode='categorical')\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    VALIDATION_DIR,\n",
    "    target_size=(150,150),\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\karan\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 148, 148, 64)      1792      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 74, 74, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 72, 72, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 34, 34, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 17, 17, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 15, 15, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               3211776   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 3,477,066\n",
      "Trainable params: 3,477,066\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    # Note the input shape is the desired size of the image 150x150 with 3 bytes color\n",
    "    # This is the first convolution\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    # The second convolution\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # The third convolution\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # The fourth convolution\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # Flatten the results to feed into a DNN\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    # 512 neuron hidden layer\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "7/7 [==============================] - 2s 263ms/step - loss: 2.2219 - acc: 0.2750\n",
      "63/63 [==============================] - 30s 483ms/step - loss: 2.4491 - acc: 0.1315 - val_loss: 2.2219 - val_acc: 0.2750\n",
      "Epoch 2/25\n",
      "7/7 [==============================] - 1s 181ms/step - loss: 1.8335 - acc: 0.3550\n",
      "63/63 [==============================] - 28s 441ms/step - loss: 2.0443 - acc: 0.2710 - val_loss: 1.8335 - val_acc: 0.3550\n",
      "Epoch 3/25\n",
      "7/7 [==============================] - 1s 177ms/step - loss: 1.1248 - acc: 0.6250\n",
      "63/63 [==============================] - 29s 455ms/step - loss: 1.7260 - acc: 0.3885 - val_loss: 1.1248 - val_acc: 0.6250\n",
      "Epoch 4/25\n",
      "7/7 [==============================] - 1s 189ms/step - loss: 1.0212 - acc: 0.6400\n",
      "63/63 [==============================] - 29s 455ms/step - loss: 1.4489 - acc: 0.5030 - val_loss: 1.0212 - val_acc: 0.6400\n",
      "Epoch 5/25\n",
      "7/7 [==============================] - 1s 176ms/step - loss: 1.1802 - acc: 0.6500\n",
      "63/63 [==============================] - 28s 445ms/step - loss: 1.1499 - acc: 0.5990 - val_loss: 1.1802 - val_acc: 0.6500\n",
      "Epoch 6/25\n",
      "7/7 [==============================] - 1s 189ms/step - loss: 0.5366 - acc: 0.8500\n",
      "63/63 [==============================] - 29s 458ms/step - loss: 0.9256 - acc: 0.6790 - val_loss: 0.5366 - val_acc: 0.8500\n",
      "Epoch 7/25\n",
      "7/7 [==============================] - 1s 174ms/step - loss: 0.2929 - acc: 0.9000\n",
      "63/63 [==============================] - 27s 435ms/step - loss: 0.7687 - acc: 0.7305 - val_loss: 0.2929 - val_acc: 0.9000\n",
      "Epoch 8/25\n",
      "7/7 [==============================] - 1s 183ms/step - loss: 0.2568 - acc: 0.9200\n",
      "63/63 [==============================] - 27s 434ms/step - loss: 0.6068 - acc: 0.7820 - val_loss: 0.2568 - val_acc: 0.9200\n",
      "Epoch 9/25\n",
      "7/7 [==============================] - 1s 177ms/step - loss: 0.2076 - acc: 0.9250\n",
      "63/63 [==============================] - 28s 443ms/step - loss: 0.5205 - acc: 0.8140 - val_loss: 0.2076 - val_acc: 0.9250\n",
      "Epoch 10/25\n",
      "7/7 [==============================] - 1s 177ms/step - loss: 0.2168 - acc: 0.9250\n",
      "63/63 [==============================] - 30s 475ms/step - loss: 0.4759 - acc: 0.8340 - val_loss: 0.2168 - val_acc: 0.9250\n",
      "Epoch 11/25\n",
      "7/7 [==============================] - 1s 192ms/step - loss: 0.1752 - acc: 0.9350\n",
      "63/63 [==============================] - 29s 459ms/step - loss: 0.4056 - acc: 0.8600 - val_loss: 0.1752 - val_acc: 0.9350\n",
      "Epoch 12/25\n",
      "7/7 [==============================] - 1s 189ms/step - loss: 0.2504 - acc: 0.9250\n",
      "63/63 [==============================] - 28s 447ms/step - loss: 0.3661 - acc: 0.8705 - val_loss: 0.2504 - val_acc: 0.9250\n",
      "Epoch 13/25\n",
      "7/7 [==============================] - 1s 190ms/step - loss: 0.1237 - acc: 0.9700\n",
      "63/63 [==============================] - 28s 452ms/step - loss: 0.3370 - acc: 0.8830 - val_loss: 0.1237 - val_acc: 0.9700\n",
      "Epoch 14/25\n",
      "7/7 [==============================] - 1s 186ms/step - loss: 0.1077 - acc: 0.9750\n",
      "63/63 [==============================] - 29s 455ms/step - loss: 0.3248 - acc: 0.8890 - val_loss: 0.1077 - val_acc: 0.9750\n",
      "Epoch 15/25\n",
      "7/7 [==============================] - 1s 195ms/step - loss: 0.0733 - acc: 0.9750\n",
      "63/63 [==============================] - 29s 459ms/step - loss: 0.2973 - acc: 0.9065 - val_loss: 0.0733 - val_acc: 0.9750\n",
      "Epoch 16/25\n",
      "7/7 [==============================] - 1s 182ms/step - loss: 0.0698 - acc: 0.9850\n",
      "63/63 [==============================] - 28s 446ms/step - loss: 0.2853 - acc: 0.9055 - val_loss: 0.0698 - val_acc: 0.9850\n",
      "Epoch 17/25\n",
      "7/7 [==============================] - 1s 182ms/step - loss: 0.0895 - acc: 0.9750\n",
      "63/63 [==============================] - 28s 449ms/step - loss: 0.2576 - acc: 0.9155 - val_loss: 0.0895 - val_acc: 0.9750\n",
      "Epoch 18/25\n",
      "7/7 [==============================] - 1s 185ms/step - loss: 0.1350 - acc: 0.9500\n",
      "63/63 [==============================] - 30s 473ms/step - loss: 0.2515 - acc: 0.9100 - val_loss: 0.1350 - val_acc: 0.9500\n",
      "Epoch 19/25\n",
      "7/7 [==============================] - 1s 190ms/step - loss: 0.0673 - acc: 0.9700\n",
      "63/63 [==============================] - 29s 456ms/step - loss: 0.2381 - acc: 0.9140 - val_loss: 0.0673 - val_acc: 0.9700\n",
      "Epoch 20/25\n",
      "7/7 [==============================] - 1s 190ms/step - loss: 0.0620 - acc: 0.9700\n",
      "63/63 [==============================] - 28s 448ms/step - loss: 0.2268 - acc: 0.9245 - val_loss: 0.0620 - val_acc: 0.9700\n",
      "Epoch 21/25\n",
      "7/7 [==============================] - 1s 187ms/step - loss: 0.0854 - acc: 0.9800\n",
      "63/63 [==============================] - 28s 448ms/step - loss: 0.1863 - acc: 0.9395 - val_loss: 0.0854 - val_acc: 0.9800\n",
      "Epoch 22/25\n",
      "7/7 [==============================] - 1s 178ms/step - loss: 0.0578 - acc: 0.9800\n",
      "63/63 [==============================] - 29s 456ms/step - loss: 0.2125 - acc: 0.9300 - val_loss: 0.0578 - val_acc: 0.9800\n",
      "Epoch 23/25\n",
      "7/7 [==============================] - 1s 184ms/step - loss: 0.1154 - acc: 0.9700\n",
      "63/63 [==============================] - 29s 455ms/step - loss: 0.1998 - acc: 0.9345 - val_loss: 0.1154 - val_acc: 0.9700\n",
      "Epoch 24/25\n",
      "7/7 [==============================] - 1s 181ms/step - loss: 0.1466 - acc: 0.9750\n",
      "63/63 [==============================] - 29s 454ms/step - loss: 0.1794 - acc: 0.9380 - val_loss: 0.1466 - val_acc: 0.9750\n",
      "Epoch 25/25\n",
      "7/7 [==============================] - 1s 197ms/step - loss: 0.0446 - acc: 0.9800\n",
      "63/63 [==============================] - 29s 454ms/step - loss: 0.1635 - acc: 0.9440 - val_loss: 0.0446 - val_acc: 0.9800\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(train_generator, epochs=25, validation_data = validation_generator, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 1 2 2 3 3 4 4 5 5 6 6 7 7 8 8 9 9]\n"
     ]
    }
   ],
   "source": [
    "images = []\n",
    "img_folder = os.path.join('C:\\\\Users\\\\karan\\\\Desktop\\\\Projects\\\\Digit recognition\\\\validation')\n",
    "img_files = os.listdir(img_folder)\n",
    "img_files = [os.path.join(img_folder, f) for f in img_files]\n",
    "# print(img_files)\n",
    "for img in img_files:\n",
    "    img = load_img(img, target_size=(150, 150))\n",
    "    img = img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    images.append(img)\n",
    "\n",
    "# stack up images list to pass for prediction\n",
    "images = np.vstack(images)\n",
    "#print(images)\n",
    "classes = model.predict_classes(images, batch_size=10)\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "model.save('m.model')\n",
    "\n",
    "print(\"Model saved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
